{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "552d82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "from pathlib import Path\n",
    "# import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d2479e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97e44d3f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac16d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "block_size = 64\n",
    "epochs = 500\n",
    "eval_epochs = 20\n",
    "info_interval = 10\n",
    "learning_rate = 1e-3\n",
    "n_channels = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21c53e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and merge all data files\n",
    "data_dir = Path(\"data/\")\n",
    "text_paths = list(data_dir.glob(\"*\"))\n",
    "\n",
    "text = ''\n",
    "\n",
    "for file in text_paths[:1000]:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        inp = f.read()\n",
    "        text += inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9c2adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:  2243746345\n",
      "rdered the Belgian First Aid and Support Team to evacuate. However, Belgian Chief Coordinator Geert Gijs, a doctor who was at the hospital with 60 Belgian medical personnel, said it was his decision to pull the team out for the night. Gijs said he requested U.N. security personnel to staff the hospital overnight, but was told that peacekeepers would only be able to evacuate the team.\n",
      "\n",
      "He said it was a \"tough decision\" but that he accepted the U.N. offer to evacuate after a Canadian medical team, also at the hospital with Canadian security officers, left the site Friday afternoon. The Belgian team returned Saturday morning.\n",
      "\n",
      "Gijs said the United Nations has agreed to provide security for Saturday night. The team has requested the Belgian government to send its own troops for the field hospital, which Gijs expects to arrive late Sunday.\n",
      "\n",
      "Responding to the CNN report that Gupta was the only doctor left at the Port-au-Prince field hospital, U.N. spokesman Martin Nesirky said Saturday that \n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset length: \", len(text))\n",
    "print(text[1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "142dba09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¤¥¦§¨©ª«¬­®¯°±²³´µ¶·¸¹º»¼½¾¿ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿĀāĂăĄąĆćĈĉĊċČčĎďĐđĒēĔĕĖėĘęĚěĜĝĞğĠġĢģĤĥĦħĩĪīĭĮįİıĲĴĵĶķĹĺĻļĽľĿŁłŃńŅņŇňŉŊŋŌōŎŏŐőŒœŔŕŖŗŘřŚśŜŝŞşŠšŢţŤťŦŧũŪūŬŭŮůŰűŲųŵŷŸŹźŻżŽžſƀƁƂƃƄƅƆƇƊƋƌƍƎƏƐƑƒƓƕƖƗƘƙƛƜƝƞƟƠơƣƤƧƨƪƬƭƮƯưƱƲƳƴƶƷƸƼǀǁǂǃǅǇǈǉǋǌǍǎǐǑǒǔǗǘǚǜǝǞǢǣǤǥǦǧǨǪǫǬǯǰǱǲǳǵǶǷǹǺǻǼǽǾǿȁȂȃȇȈȉȊȋȌȍȎȏȐȑȒȓȔȕȖȗȘșȚțȜȝȞȟȡȤȦȧȯȰȲȳȶȻɁɂɃɈɉɊɋɌɍɎɏɐɑɒɓɔɕɖɗɘəɚɛɜɝɟɠɡɢɣɤɥɦɧɨɪɫɬɭɮɯɰɱɲɳɴɵɶɸɹɺɻɽɾʀʁʂʃʄʆʇʈʉʊʋʌʍʎʏʐʑʒʔʕʖʗʘʙʛʜʝʞʟʠʡʢʣʤʧʨʯʰʱʲʶʷʹʺʻʼʽʾʿˀˁ˂ˆˇˈˊˋˌːˑ˔˕˖˘˙˚˜˝˞˟ˠˢˣˤ˥˦˧˨˩ˮ˰˴˺̴̵̶̷̸̡̢̧̨̛̖̗̘̙̜̝̞̟̠̣̤̥̦̩̪̫̬̭̮̯̰̱̲̳̹̺̻̼͇͈͉͍͎̀́̂̃̄̅̆̇̈̉̊̋̌̍̎̏̐̑̒̓̔̽̾̿̀́͂̓̈́͆͊͋͌̕̚ͅ͏͓͔͕͖͙͚͐͑͒͗͛ͣͤͥͦͨͩͪͫͬͭͮͯ͘͜͟͢͝͞͠͡Ͱʹ͵ͺͻͼͽ;΁΂΃΄ΆΈΉΊΌ΍ΎΏΐΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩΪΫάέήίΰαβγδεζηθικλμνξοπρςστυφχψωϊϋόύώϏϐϑϒϓϕϖϗϜϝϟϡϢϬϰϱϲϴϵ϶ϹϽϾϿЁЂЃЄІЇЈЉЊЋЌЍЎЏАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяѐёђѓєѕіїјљњћќѝўџѠѡѣѥѦѧѩѫѭѯѰѱѲѴѵѷѸѼҁ҂҉ҌҏҐґҒғҚқҜҡҢңҫҬҮүҰұҳҵҷӀӁӂӉӌӑӒӕәӟӠӣӦӧӨөӴԁԂԃԋԌԍԎԏԒԗԙԛԟԦԱԲԳԴԵԶԷԸԹԺԻԼԽԾԿՀՁՂՃՄՅՆՇՈՉՊՋՌՍՎՏՐՑՒՓՔՕՖ՘՛՜՝՞ՠաբգդեզէըթժիլխծկհձղճմյնշոչպջռսվտրցւփքօֆև։֊֎֏ְֱֲֳִֵֶַָֹֻּֽ֑֛֣֤֥֦֭֗֝֩֫־ֿׁׂ׃ׇ׉אבגדהוזחטיךכלםמןנסעףפץצקרשתױײ׳״׻׿؂؈؋،ؙؐؑ؛؝؟ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيًٌٍَُِّْٓٔٙٚ٠١٢٣٤٥٦٧٨٩٪ٰٱٹٺپٿځڂڃچڈډڋڌڍڎڏڐڑڒړڕژښڞڢڤکڪڭگڳڵںھہۂۃۆۇیێۏېےۓ۔ەۖۗۙۚ۞۟ۦ۰۱۲۳۵۶۷ۺۿ܂܆܊ܐܒܓܕܖܗܘܙܚܛܝܟܠܡܢܣܥܦܨܩܪܫܬܯܸܹܼ݂݆݈ܰܳܵܶܺܽܿ݀݁݃݇݊݋݌ݍݏݐݒݖݗݙݜݭݸݽހށނރބއވމދތލސޕޗޘޝޟޠޡަާިީުެޭޯް޴޻߁߂߆߇ߊߋߌߏߑߒߓߔߕߘߛߜߟߥߪ߬ߵ߷߾ँंःअआइईउऊऋऎएऐओऔकखगघङचछजझञटठडढणतथदधनपफबभमयरलळवशषसह़ऽािीुूृॅॆेैॉोौ्ॐफ़।॥०१२३४५६७८॰ঁংঅআইঈউএওকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহ়ািীুৃেৈোৌ্ৎৰৱ৷ਂਅਆਇਈਉਏਐਓਕਖਗਚਛਜਟਠਡਢਣਤਥਦਧਨਪਬਭਮਯਰਲਵਸਹਾਿੀੁੂੇੈੋ੍੧੨੩੫੭ੰੱੴઍગજતદનપભરલળવષસહાિીુેો્ଂକଠଣତଥଦଧନପବମରଳସାିୀ୍ୟஅஇஊஏஐகஙசஜஞடணதநனபமயரறலளழவஷஸாிீுூெேைொோ்ంఏకఖగఘచజటడణతదధనపభమరలళవశసాిీుూెేొోౌ్ಂಅಏಕಗಚಜಞಠಡಣತಥದಧನಪಬಭಮಯರಲಳವಶಷಸಾಿೀುೆೇೊೋ್ംഅഇഏകഗചജഞടണതദനപബഭമയരറലളവശഷസാിീുൂെോ്൬ൻൾංආඒකගචජඩණතදනපබමයරලවසහ්ාිීුෙොෝกขคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลวศษสหอฮฯะัาำิีึืุู฿เแโใไๆ็่้๊๋์๑๓๔๕๙ຈຊດຕທນບປພຟລວສະັາິີົເໄ໒༆༉་༼༽ཀཁགངཆཇཏཐདནཔཕབམའརལསཧིེོུྐྒྭྱྲငစညတထနပဘမရသအဧါာိုူေံ့း္်ျြွႢႤႦႵაბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰᄅᆎህለሊላልሎሐሕመማምሪርሱስቋቤብተቲትቶናንኛአኢኦኬክወዋዐየያዮደዳድዶጅገጌጎጥጵጽጾፊፐፒ፭ᎦᎩᎾᏏᏒᏔᏕᏗᏝᏦᐃᐅᐊᐋᐎᐖᐗᐙᐛᐠᐢᐦᐧᐯᐱᑎᑐᑕᑦᑰᑲᒃᒋᒍᒥᒪᒻᓂᓄᓇᓐᓕᓛᓯᓴᔉᔊᔕᔵᔺᕆᕈᕋᕐᕕᕗᕙᕤᕦᕿᖁᖄᖅᖇᖏᖴᗙᗜᗦᗩᗪᗰᙠᚢᚦᚨᚩᚱᚳᚷᚹᚻᚾᛁᛈᛋᛖᛗᛚᛞᛟᛰᛱᛴᜉᜎᜓកខគងញដឋណទធនបពមរលវសហឡាីូេែៅ៉្᠎ᠠᠢᠣᠤᠨᠩᠪᠮᠯᠰᠳᠵᠶᠷᠸᡠᡡᡤᡥᡩᡳᡴᡵᨓ᪈ᴀᴄᴅᴇᴉᴊᴋᴏᴗᴘᴛᴠᴡᴤᴥᴬᴳᴴᴵᴷᴸᴹᴺᴼᴾᵃᵇᵎᵏᵐᵑᵒᵔᵗᵠᵩᵹᶅᶥᶮᶲᷓḂḇḊḌḍḎḏḐḗḞḡḤḥḪḫḰḱḲḳḴḵḶḷḹḻḾṀṁṃṅṆṇṉṓṖṙṚṛṝṟṠṢṣṥṪṬṭṯṳṵṼẊẒẓẖẞạảấầẩậắằẳẵặẹẻẼẽếềểễệỉịỌọỏốồổỗộớờởỡợụủỨứừửữựỳỵỷỹἀἁἂἃἄἅἇἈἉἌἍἐἑἓἔἕἘἙἠἡἢἣἤἥἦἧἩἮἰἱἳἴἵἶἷἸἹὀὁὃὄὅὈὉὌὐὑὔὕὖὗὡὤὥὧὩὮὰάὲέὴήὶίὸόὺύὼώᾔᾰᾱᾳᾶᾷ᾽᾿ῃῆῇῐῑῖῠῡῥῦῧῬῳῴῶῷ῾​‌‍‎‏‐‑‒–—―‖‘’‚‛“”„‟†‡•‣․…‧‪‫‬‭‮‰‱′″‴‵‶‷‸‹›※‼‽‾‿⁂⁃⁄⁉⁊⁕⁠⁡⁢⁬⁰ⁱ⁴⁵⁶⁷⁸⁹⁺⁻⁽⁾ⁿ₀₁₂₃₄₆ₐₒₓ₠₡₢₣₤₥₦₧₨₩₪₫€₭₮₯₰₱₲₳₴₵₷₹⃁⃣ℂ℃℅℉ℊℋℌℍℎℏℐℑℒℓℕ№℗℘ℙℚℛℜℝ℞℠™ℤΩℨKℬℭ℮ℯℰℱℳℴℵℶℹⅈ⅋⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩⅪⅫⅬⅭⅮⅯⅰⅱⅲⅳⅴⅵⅶⅷⅸⅹⅺⅻⅼⅽⅾⅿↀↃ←↑→↓↔↕↖↗↘↙↛↦↧↩↪↴↵↺↻↼↽⇀⇂⇄⇆⇌⇏⇐⇑⇒⇓⇔⇘⇚⇛⇠⇢⇧⇨⇩∀∂∃∄∅∆∇∈∉∋∎∏∑−∓∕∗∘∙√∝∞∟∠∣∥∦∧∨∩∪∫∬∭∮∴∵∶∷∼≃≅≈≌≜≠≡≤≥≦≧≪≫≲≳≻⊂⊃⊆⊊⊏⊕⊖⊗⊙⊢⊣⊤⊥⊨⊭⊮⊸⊹⊿⋂⋄⋅⋆⋈⋋⋕⋮⋯⌀⌂⌈⌊⌋⌐⌒⌘⌚⌛⌥⌦⌨〈〉⌫⌴⌽⍜⍝⍵⎇⎑⎝⎠⎯⎼⏎⏕⏞⏟⏤⏩⏪⏫⏬⏭⏰⏱⏳␇␢␣␭①②③④⑤⑥ⒶⒷⒸⓂⓇⓉⓋⓍⓐⓒⓔⓘⓚⓛⓝⓡ─━│┃┌┏┐┓└┕┗┘┛├┣┤┫┬┳┴┻┼╋═║╒╔╕╗╚╜╝╠╣╥╦╨╩╬╭╮╯╰╱╲╳╹▀▄▅▆▇█▉▋▌▍▐░▒▓■□▢▣▪▫▬▮▰▲△▴▵▶▷▸▹►▻▼▽▾▿◀◁◂◃◄◅◆◇◈◉◊○◌◎●◓◔◕◘◙◡◢◣◤◥◦◧◨◫◯◻◼◽◾☀☁☂☃☄★☆☇☈☉☎☏☐☑☒☓☔☕☖☘☙☚☛☝☞☠☤☥☦☧☨☩☪☬☭☮☯☰☱☲☳☴☵☷☸☹☺☻☼☽☾☿♀♁♂♄♆♈♉♊♋♌♍♎♏♐♑♒♓♔♕♖♗♘♙♚♛♜♝♞♟♠♡♢♣♤♥♦♧♨♩♪♫♬♭♮♯♰♻♿⚀⚁⚂⚃⚄⚅⚐⚑⚒⚓⚔⚖⚗⚙⚛⚜⚠⚡⚥⚪⚫⚰⚽⚾⛄⛅⛎⛓⛔⛪⛲⛳⛵⛷⛺⛽✁✂✅✆✇✈✉✊✋✌✍✎✏✐✑✒✓✔✕✖✗✘✙✝✞✟✠✡✢✣✤✥✦✧✨✩✪✫✬✭✮✯✰✱✲✳✴✵✶✷✸✹✺✻✼✽✾✿❀❁❂❃❄❅❆❇❈❉❊❋❌❍❎❑❓❔❕❖❗❛❝❞❣❤❥❦❧❮❯❶❷❸➔➕➖➗➙➜➝➡➢➤➥➨➫➯➰➳➷➹➻➿⟦⟧⟨⟩⟲⟳⟶⟷⟹⟺⠀⤴⤵⤻⦁⨁⨂⨯⩱⩲⩽⩾⩿⪀⪆⬅⬆⬇⬛⬜⬥⭐⭕⯀ⱱⱷⱺⲁⲂⲅⲉⲏⲐⲑⲓⲗⲙⲛⲟⲠⲡⲣⲥⲧⲩⲪⲫⲬⲱⴰⴱⴻⵄⵉⵍⵎⵏⵓⵙⵜⵟⵡ⸢⸣⸮⺈⺊⺌⺍⺗⺮⺻⻊⻌⻏⻖⻗⿰⿱⿴⿸⿺、。〃々〆〇〈〉《》「」『』【】〒〓〔〕〜〝〟〰〽ぁあぃいぅうぇえぉおかがきぎくぐけげこごさざしじすずせぜそぞただちぢっつづてでとどなにぬねのはばぱひびぴふぶぷへべぺほぼぽまみむめもゃやゅゆょよらりるれろゎわゐゑをん゛゜ゝゞァアィイゥウェエォオカガキギクグケゲコゴサザシジスズセゼソゾタダチヂッツヅテデトドナニヌネノハバパヒビピフブプヘベペホボポマミムメモャヤュユョヨラリルレロヮワヲンヴヵヶ・ーヽヾㄅㄆㄇㄈㄉㄊㄋㄌㄍㄎㄏㄐㄑㄒㄓㄔㄕㄖㄗㄘㄙㄚㄛㄜㄝㄞㄟㄠㄡㄢㄣㄤㄥㄧㄨㄩㄱㄷㄹㅁㅂㅅㅆㅇㅉㅋㅌㅎㅜㅠㅡㅤㆴ㇁㊗㊙㋉㋡㎜㎞㎡㐢㞎䂦䂭䑤䒑䗦䞼䦃䳧䳴一丁七万丈三上下不与丑专且世丘丙业丛东丝丞丟両丢两严並丧个丫中丰串临丶丷丸丹为主丼丽举丿乃久么义之乌乍乎乏乐乔乗乘乙九乞也习乡书买乱乳乼乾亀亂亅了予争事二于亏亐云互五井亚些亜亞亠亡交亦产亨享京亭亮亲人亻亿什仁仅仆仇今介仍从仏仑仔仕他仗付仙代令以仪们仮仰仲件价任份仿伀企伊伍伎伏伐休众优伙会伝伞伟传伤伦伪伯估伴伶伸伺似伽佃但佇佈位低住佐佑体何佗余佛作你佣佬佳併使來例侍侏侘供依侠価侣侦侧侨侯侵侶便係促俄俊俗保俞俠信修俯俱俳俵俺倉個倍們倒倘候倚借値倦倩倪倫倭倶值倾假偈偉偏做停健側偶偷偽偿傅傍傑傘備傤储傫催傲傳傷傻傾僅働像僑僕僚僧僱僵僻儀儂億儉儒償優儲儿允元兄充兆先光克兌免児兑兒兔兖党兜入內全兩八公六兮兰共关兴兵其具典养兼兽冀冂内円冈冊册再冑冒冕冖写军农冠冤冥冨冫冬冯冰冲决况冶冷冻净凄准凉凌凍减凛凝几凡凤処凭凯凰凱凵凶凸凹出击函凿刀刂刃分切刊刑划刔刖列刘则刚创初判別利别到制刷券刹刺刻剃則削剋前剑剛剝剣剤剥剩副剰割創劃劄劇劈劉劓力劝办功加务劣动助努劫励劲劳労効势勃勅勇勉勋勒動勘務勝勞募勢勤勧勳勹勺勾勿匁匂包匕化北匙匚匠匣匪匹区医匿區十千升午半卌卍华协卐卑卒卓協单卖南単博卜占卡卢卦卧卩卫卯印危即却卵卷卸卻卿厂厄历压厌厏厚厝原厢厥厭厳去县参參又叉及友双反収发取受变叟叡叢口古句另叩只叫召叭可台叱史右叶号司叹吃各合吉吊同名后吏吐向吓吗君吞吟吠否吧吨吩含听启吳吴吶吸吹吼吾呂呃呆呈告呐员呛呟呢周呪呲味呸呼命咆咋和咎咏咐咒咔咕咖咤咪咫咬咯咲咸咻咽哀品哇哈哉响哎哑哗員哥哦哪哭哮哲唄唆唇唐唔唖唤售唯唱唼唾啄商啊問啓啡啤啦啵喀善喆喇喉喋喔喘喚喜喝喧喩喪喫喬單喰喱喵営喻嗅嗎嗔嗚嗟嗡嗬嗯嘀嘉嘗嘘嘚嘛嘞嘢嘩嘱嘴嘿噂噌噛噢器噴噺噻嚇嚕嚢嚣嚴嚼囊囗四回因团団园囯困囲図围固国图圆圈國圍圏園圓圖團土圣圧在圭地圳场圾址坂均坊坎坏坐坑块坚坛坝坡坤坦坪垂垃型垛垢垦埋城埔域執培基埼堀堂堃堅堡堤堪堯報場堵塊塑塔塗塚塞塡塩填塵塾境墓増墙墚墜增墟墨墮墳墻墾壁壇壊壌壞壩士壮声売壹壽夂处备変复夏夕外多夜够夠夢大天太夫夬央失头夷夹夺夾奄奇奈奉奋奎奏奐契奔奕奖套奥奧奪奮女奴奶她好如妃妄妇妈妊妍妒妓妖妙妨妬妮妹妻妾姆姉始姐姑姓委姚姜姥姦姨姫姻姿威娃娅娇娖娘娜娠娱娼婆婊婕婚婦婭婷婿媒媚媛媳媺媽嫁嫉嫌嫖嬉嬌嬢嬪嬴嬸子孔字存孙孜孝孟季孤学孩孫學宀宁它宅宇守安宋完宏宓宗官宙定宛宜宝实実审客宣室宥宦宫宮宰害宴宵家容宽宾宿寂寄密寇富寒寛寝察寡寢實寧寨審寮寵寶寸对寺寻导対寿封専射将將專尉尊尋對導小少尔尖尘尚尛尝尤就尸尹尺尻尼尽尾尿局屁层屄居屆屈届屋屌屍屎屏展属層履屬屮屯山屿岁岐岗岛岡岩岬岭岳岸峙峠峡峨峪峯峰島峻崇崎崔崙崩崽嵐嵩嶄嶋嶺嶽巛川州巡巢巣工左巧巨巩巫差己已巴巷巻巽巾币市布帅帆师希帖帜帝帥带師席帮帯帰帶常帼帽幅幌幕幟干平年并幸幹幺幻幼幽幾广庁広庄庆庇床序庐库应底店庙庚府庞废度座庫庭庶康庸廃廉廊廓廖廟廠廣廬廳廴延廷建廻廿开弁异弃弄弊弋式弓引弘弟张弥弦弧弩弱張強弹强弾彊彎彑归当录彗彙彡形彦彩彪彬彭彰影彳彷役彻彼往征径待很律後徐徑徒従得從御徨復循微徳徴徵德徹徽心忄必忆忌忍志忘忙応忠忧快念忽怀态怎怒怕怖思怠怡急性怨怪怯总恋恐恒恕恠恢恥恨恩恭息恰恳恵恶恼恽悄悉悍悔悟悠患悤您悩悪悬悭悲悵悻悼情惇惊惋惑惜惟惠惡惧惨惫惬惭惯惰想惹惺愁愈愉意愕愚愛感愤愧愿慄慈態慌慎慕慘慢慣慧慨慮慰慶慾憂憎憐憑憤憧憬憲憶憾懂懇應懐懲懷懸懿戀戈戉戎戏成我戒或战戚戟戦截戯戰戲戳戴戶户戸戻房所扁扇扈扉手扌才扎扑扒打払托执扩扫扬扰扱扶批找承技抄抉把抑抓投抖抗折抜択抢护报披抬抱抵抹押抽拂担拆拉拍拐拒拓拔拖拗拘拙招拜拝拟拠拡拥拦拨择括拭拯拳拶拼拾拿持挂指按挑挙挝挡挣挤挥挨挫振挺捅捉捋捌捎捐捕捜损换捨据捲捷掀授掉掌排掘掛掠採探接控推掩掬掲掴掷掻揃揉揍描提插揚換握揭揮援揺搅損搏搜搞搬搭搶携摂摄摆摇摘摩撃撑撒撕撤播撮撰撸撾擁擂擅擇擊操擎擒擔據擞擢擦擬擴擼攘攝支攵收改攻放政故效敌敏救敖敗教敛敝敞敢散敦敪敬数整敵敷數文斉斋斌斐斑斗料斜斤斧斩斬断斯新斷方於施旁旅旋族旗无既日旦旧旨早旬旭旱时旺昂昆昇昌明昏易昔昕星映春昧昨昭是昼显時晃晉晋晒晓晕晚晞晨晩普景晰晴晶智暁暂暇暑暖暗暦暫暮暴曆曖曙曜曝曰曲更書曹曼曾替最會月有朋服朕朗望朝期朦朧木未末本札术朱朴朵机朽杀杂权杉李杏材村杖杜束杠条杢来杨杭杯杰東松板极构析枕林枘枚果枝枠枪枭架柄柏某柒染柔查柯柱柳柴査栄栅标树栖栗校株样核根格桂桃桅框案桐桑桓桜桥桨桩桶梁梅梓梗梘條梟梢梦梨梭梯械检棄棉棋棍棒棕棟棠森棲椅植椎椒椛検椭椰楊楓楔楚業極楼楽概榎榔榛榜榮榴構槌槍槐様槽樂樋樓標模樣樨権横樫樱樹樺樽橋橐橘橙橚機檀檎檔檛檢檻櫻欄權欠次欢欣欧欲欺欽款歇歉歌歐歓歡止正此步武歧歩歪歯歲歳歴歷歹死歼殉殊残殖殴段殷殺殻殿毀毁毅毋母毎每毒毓比毕毘毙毛毫氏民气気氣氧氫水氵氷永氺汁求汉汐汕汗汚汝江池污汤汪汰決汽汾沁沂沃沈沉沒沖沙沟没沢沦沪沫河沸油治沼沿況泄泉泊泓法泞泡波泣泥注泰泳泽洁洋洗洛洞津洪洱洲活派流浅浊测济浑浓浙浜浦浩浪浬浮浴海浸涂涅消涉涌涙涛润涧涩涯液涵涼淀淑淘淚淡淦淨淫深淳混添清渇済渉渋渐渓渔渗減渠渡渥渦温測港渴游渺渾湄湖湘湛湧湯湾湿満溃溏源準溜溝溢溥溪溶溺滅滋滌滑滓滕滚滝滞满滨滩滬滲滴滿漁漂漆漉漏演漕漠漢漤漪漫漬漯潇潔潘潜潟潤潮潰澄澎澤澪澳激濃濟濡濤濫濱濺瀏瀚瀬灘灣火灬灭灯灰灵灶灼災炁炉炎炒炜炢炤炩炫炭炮炯炷炸点為炼烂烈烏烜烟烤烧热烯烺烽焉焚無焦然焼煌煎煒煙煤煥照煮煽熄熊熙熟熠熱燃燈燒燕營燦燧燭燮爆爐爛爪爬爭爱爲爵父爷爸爺爽爾牆片版牌牙牛牟牢牧物牲牵特牺牽犀犠犬犭犯状狀狂狐狒狗狙狡狩独狭狱狸狼狽猎猛猜猟猥猪猫献猴猶猷猾猿獄獅獎獒獣獨獲獸獻玄率玉王玖玛玟玥玩玫环现玲玻珊珍珠班現球琅理琉琊琚琢琦琪琰琳琴琵琼瑄瑊瑋瑚瑛瑜瑞瑠瑤瑩瑪瑰瑶瑽璃璋璐璧環璼瓊瓚瓜瓢瓦瓶瓷甄甘甚甜生產産甥用甩甫甭田由甲申电男甸町画畅界畑留畚畜畝畢略番畫異畳當畿疆疇疈疑疒疗疫疮疯疱疲疹疼疾病症痊痔痛痩痴瘋瘍瘟瘢瘾療癌癒発登發白百皃的皆皇皈皓皞皮皿盂盃盆盈益盐监盒盖盗盘盛盟監盤盧目盯盲直相盾省眉看県真眠眨眩眷眺眼眾着睛睡督睦睿瞋瞎瞑瞧瞪瞬瞭瞰瞳矍矛矜矢矣知矫短矮矯石矶码砂砍研砕砧砲破砸础硫硬确碁碌碍碑碗碟碧碩碰碱確碼碾磁磊磐磨磯礁礎示礻礼社祇祈祉祐祓祖祝神祠祥票祭祷祸祺祿禁禄禅禊禍禎福禕禦禧禪禮禸禹禺离禾秀私秉秋种科秒秘租秦秩积称移秽稀程稍税稚稜種稱稲稳稷稻稼稽稿穀穂穆積穎穏穢穩穫究穷穹空穿突窃窑窒窗窘窝窟窥窦窮立竖站竜竞竟章竣童竪端競竹笊笑笔笛笞笠符笨第笹笼筆等筋筑答策筥筴签简箅箇算箝管箫箱箸節範築篌篝篤簋簟簡簽簾籃籌籍籠米类粉粋粒粗粛粟粤粪粵粹精糊糕糖糞糟糧糸系紀約紅納紐純紙級紛素索紧紫累細紳紹終組絆経結絕絞絡絢給統絲絵絶絹經継続綜綠維綱網綴綺綾綿緊総緑緒線締緣編緩緯練緹縁縄縈縛縣縮總績繁繊繋織繭繰繳繼續纏纠红约级纪纯纲纳纵纷纸纹纽线练组细织终绍经结绕给络绝统绣绥继绩绪续绮绯绳维绷综绿缇缓编缘缠缩缶缸缺缽罄网罒罕罗罠罢罪置罰署罵罹羅羊美羞群義羽翁翅翌習翔翠翰翻翼耀老考者而耐耒耕耗耳耸耻耽聆聊聋职联聖聘聚聞聯聲聴職聽聿肆肇肉肋肌肏肚肛肝肠股肢肥肩肪肭肯育肳肵肷肺胃胆背胎胜胞胡胧胯胱胴胸能脂脅脆脈脊脏脑脚脩脱脳脸脹腎腐腔腕腚腦腫腰腸腹腺腾腿膀膏膚膜膝膠膳膵膺膾臉臓臘臚臣臥臨自臭至致臺臼舂舆與興舉舊舌舎舒舔舗舛舜舞舟航般舰舷船艘艤艦良色艶艸艹艺艾节芃芋芒芝芦芭芮芯花芳芷芸芽苇苍苏苑苗苟若苦英苹苺茁茂范茅茎茜茨茫茵茶茸茹茾荆荇草荐荒荘荞荠荡荣药荷莉莖莞莫莱莲获莼莽菅菇菊菌菓菕菜菟菠菩華菱菲萃萄萊萌萍萎萝营萨萩萬落葆葉著葛葡董葤葦葫葬葵葽蒂蒋蒐蒙蒜蒭蒲蒸蒼蓄蓋蓝蓬蓮蓾蔑蔓蔡蔣蔲蔵蔷蔽蕃蕉蕗蕙蕡蕭蕴蕾薄薇薛薦薩薪薫薬薯藉藍藏藝藣藤藥藧藩藻蘇蘋蘑蘭虎虐虑處虚號虫虹虽虾蚁蚂蚕蛇蛋蛍蛎蛙蛛蛮蛰蛹蜀蜂蜘蜜蜡蝙蝟蝠蝦蝴蝶融螞螺蟠蟬蟲蟹蟻蟾蠅蠛蠢蠻血衆行術衔街衛衝衡衣衤补表衰衷袁袋袍袖被裁裂装裏裔裕裘裙補裝裡裳裴裸製裾複褐褒襪襲西覀要覆覇見規視覚覧親観覺覽觀见观规视览觉角解触言訂計訊討訓託記訝訟訪設許訳訴診註証詐評詞詠試詭詮詰話該詳詹誇誌認誐誓誕誘語誠誤說説読誰課調談請諏論諜諦諳諸諺諾謀謂謉謎謙講謝謡謨證識譜警譬譯議譲護讃變讓讚计订认讨让训议讯记讲讶许论设访诀证评识诉诊词试诚话诡询该详语误诲说诵诶请诸诺读课谁调谈谋谎谐谓谢谣谩谬谱谴谷谿豆豊豌豐豚象豪豬豸豹貌貍貓貝貞負財貢貧貨販貪貫責貰貴買費貼貿賀資賈賊賑賓賛賜賞賢賣賦質賬賭賴購賽贅贈贝负贡财责贤败账货质贩贪贬购贯贱贴贵贸费贼贾资赋赌赏赚赛赞赠赢赤赦走赵赶起超越趋趙趣足趴跃跋跑距跟跡跣跨路跳践踊踏踒踢踪蹟蹦蹬蹲蹴躁躇躊躍身躯躱車軌軍軒軟転軽較載輔輕輝輟輩輪輯輸轉轟车轨轩转轮软轻载较辅辆辈辉辑输辖辛辞辟辣辤辦辨辩辭辯辰辱農辶边辺込达迂迄迅过迈迎运近返还这进远违连迟迤迦迪迫迮述迳迷迹追退送适逃逆选透逐递逓途逗這通逛速造逢連逮週進逶逸逻逼遂遅遇遊運遍過遐道達違遗遙遜遞遠遡遣遥適遭遮遲遵遷選遺避邀還邊邏邑邓邢那邦邩邪邮邱邳邸邽郁郂郃郆郊郌郍郎郑郝郡部郭郷都鄂鄉鄒鄙鄧鄭酉配酒酔酢酬酱酵酷酸醂醉醋醒醜醤醫醬醸釆采釈释釋里重野量金釗釜針釣鈉鈍鈞鈡鈴鉀鉄鉅鉱鉴鉿銀銃銅銆銑銖銘銚銜銭銳鋒鋙鋼錄錘錢錦錨錫錬錮錯録鍊鍋鍑鍔鍛鍬鍮鍵鍾鎌鎎鎖鎚鎞鎧鎮鏈鏞鏡鐘鐭鐳鐵鐸鑀鑑鑫鑼鑽针钉钓钟钢钥钧钱钻铁铄铆铜铬铭银铸链销锅锋锐错锚锡锤键锻镇镔镜長长門閃閉開閎閏閑間閔関閣閩閱閲閻闆闇闕闘關闝闢门闪闭问闲闳间闷闻闽阁阅阙阜阝队阪阱防阳阴阵阶阻阿陀附际陆陈陌降限陛院陣除陥险陪陰陳陵陶陷陸険陽隅隆隊隋階随隐隔隙際障隠隣隨險隱隶隷隸隹隻隽难雀雁雄雅集雇雉雋雌雍雑雕雖雙雜雞離難雨雪雫雯雰雲零雷雹電雾需霄霆震霊霏霖霜霧露霸靈靑青靓靖静非靠面革靴鞋鞠鞭韋韑韓韩韭音韶韻響頁頂頃項順須預頑頒頓領頬頭頰頸頻頼題額顏顔願類顧顯页顶项顺须顽顾顿预领频颖颗题颜额風风飘飙飛飞食飠飢飥飩飯飲飴飼飽飾餃餅養餌餐餓餘館餺饂饃饅饑饗饭饮饱饺饼馆馍馒首香馬馴駄駅駆駈駐駿騎騒験騙騰騷驗驚驟驢驪马驰驳驼骁骆验骏骑骗骚骥骨骰骸髄髓體高髙髢髪髻鬥鬧鬪鬯鬲鬼魁魂魄魅魏魔魚魯鮎鮑鮮鯉鯨鰐鱗鱼鱿鲁鲍鲛鲜鲵鲸鳖鳥鳩鳳鳴鴉鴛鴨鴻鵜鵬鵲鶏鶜鶴鷄鷹鷺鸞鸟鸡鸣鸦鸽鸿鹄鹑鹰鹺鹿麒麓麗麟麥麦麹麺麻麼麽麿黃黄黍黎黑黒默黙點黥黨鼈鼎鼓鼠鼻齊齋齐齒齡齢龃龄龅龉龍龐龔龙龜龟ꂤꂪꂱꂵꂼꂽꇩꉸꊛꌗꍇꍏꎟꏊꏏꏟꐢ꒍꒳ꕔꙊꙌꙑ꜂ꜣꜥꝐꝒꝛꝜꝩꝺꝼꝽꝾꝿꞇ꞉Ᶎꠁꠃꠅ꠆ꠉꠌꠎꠑꠖꠘꠡꠤꠥꠦꦁꦂꦈꦏꦔꦛꦝꦟꦠꦢꦤꦥꦧꦩꦫꦭꦮꦱꦲꦴꦶꦸꦺꦼ꧀꧉꧋가각간갈감갑갓갔강갖같개객갤걔거걱건걸검겁것게겐겟겠겨격견결겹겼경곁계고곡곤곧골곰곱곳곴공과곽관광괘괜괴굉교구국군굳굴궁권귀규균그극근글금급긋기긱긴길김깃깊까깎깐깔깜깝깨꺼꺽껀껄껍껏께껴꼬꼭꼴꼼꼽꽃꽤꾀꾸꾼꿀꿉꿔뀌끄끈끊끌끔끝끼끽낀낄낌나낙낚난날낤남났낮낳내낵낸낼냄냈냉냐냥너넉넌널넓넘넛넝넣네넥넨넬넷넼녀녁년념녔녕노녹논놀놈농높놓놔놨누눈눌뉘뉴늄느늑는늘능늦니닉닌닐님닛다단닫달닭담답닷당대댁댄댈댓더덕던덜덤덧덮데덱덴뎅뎌도독돈돌돕동돼됐되된될됨됩두둑둔둘둥둬뒤뒷듐드득든듣들듬듯등디딕딘딨딩딪따딱딸땅때땐떠떡떤떨떳떻떼또똑똘뚜뚝뚫뚱뛰뜨뜩뜻라락란랄람랍랐랑래랙랜램랩랫랬랭략량러럭런럴럻럼럽렀렁렇레렉렌려력련렬렵렷렸령례로록론롤롭롯롱뢰료룡루룩룰룸룹뤄류률륨륭르른를름릅릉리릭린릴림립릿링마막만많말맘맙맛망맞맡매맨맺머먹먼멀멋멍메멘멜며면멸명몇모목몬몰못몽묘무묵문묻물뭐뭔뭘뮤므미민믿밀밍및바박밖반받발밝밥방밭배백밴뱀뱃버벅번벌범법벗베벤벨벽변별병볕보복볶본볼봄봅봉봐봣봤뵙부북분불붉붙뷔뷰브븐블비빈빌빙빚빛빠빤빨빵뻣뻥뽀뽑뾳뿐뿔뿜쁘쁜삐사삭산살삶삼샀상새색샌샘생서석선설섬섭섯섰성세센셈셉셔션셧셨소속손솔솟송쇄쇠쇼수숙순술숨숲쉐쉬쉽쉿슈스슨슬슴습슷승시식신실싫심십싶싸싼쌈쌍쌓써썩썹썼쎄쏘쐠쐬쓌쓰쓴쓸씀씨씩씬씸씹아악안앉않알암압앗았앙앞애액앨앱야약얀양얘어억언얻얼엄업없엇었엉에엑엔엘엣여역연열염엿였영옆예옛오옥온올옮옴옵옷옹와완왔왕왜외요욕용우욱운울움웁웃웅워원월웟웠웨웬웹윁위윈윗윙유육윤율으은을음읍응의이익인일읽임입잇있잉잊잎자작잔잖잘잠잡장재쟁쟤저적전절젊점접정젖제젝젤져졌조족존졸좀종좆좋좌죄죠죤주죽준줄줌중줘줬쥐쥬즈즉즌즐즘증지직진질짐집짓징짜짝짤짧째쨋쨌쨍쩔쪽쫓쭉쯤쯧찌찍찔찝차착찬찮찰참창찾채책챔챙첆처척천철첨첩첫청체쳐쳤초촌촛총최추축춘출춤충춰취츄츠측층치칙친칠침칩칭카칸캐캠캡커컨컬컴케켓켜켰코콘콜콤콧콩콸쾅쾌쿠쿨쿵퀄퀘퀴큐크큰클큼킢키킨킬킯킴킵킸킹킽타탁탄탈탐탑탕태택탠탭터턴털텀테텍텐텔템토톤톱통퇴투튀튜트특튼틀틈틉티틱틴틸팀팅파판팔팜팝팡패팩팬팽퍼페펙펜펞편펼평폐포폭폰폴폼푈푊표푸풀풆품풍퓬프픇픈플픔피픽핀필핑하학한할함합항해핵핸햄햇했행향허헌험헝헤헥헪혀혁현혐협혔형혜호혹혼홀홈홍화확환활황회획횟효후훈훗훨훼휘휠휩휭휴흑흔흘흡흥희흰히힌힐힘힙女﨑辶ﬀﬁﬂﬃﬄﭑﭒﭓﭔﭕﭖﭗﭘﭙﭚﭛﭜﭝﭢﭮﭯﭰﭱﭲﭳﭴﭵﭶﭷﭸﭹﭺﭻﭼﭽﮍﮎﮏﮐﮑﮒﮓﮔﮕﮖﮗﮘﮢﮣﮤﮥﮦﮧﮨﮩﮪﮫﮬﮭﮮﮯﮰﮱﯓﯔﯕﯖﯗﯘﯙﯚﯛﯜﯝﯞﯟﯠﯡﯢﯣﯤﯥﯦﯱﯲﯳﯴﯵﯶﯷﯸﯹﯺﯻﯼﯽﯾﯿﰀﰁﰂﰃﰄﰅﰆﰇﰈﰉﰊﰋﰌﰍﰎﰏﰐﰑﰒ﴾﴿ﷆ﷌﷟ﷺ﷼︁︎️︠︡︵︶︿﹏﹠ﺀﺃﺅﺉﺍﺏﺕﺙﺝﺡﺥﺩﺫﺭﺮﺯﺱﺵﺹﺽﻁﻃﻅﻉﻍﻑﻕﻙﻝﻟﻡﻥﻧﻩﻭﻮﻲ﻿！＃＄％＆＇（）＊＋，－．／０１２３４５６７８９：；＜＝＞？＠ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ［＼］＾＿｀ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚ｛｜～｡｢｣､･ｦｧｨｪｬｯｰｱｲｴｵｶｸｺｼｽｿﾀﾂﾃﾄﾆﾉﾊﾋﾌﾍﾎﾐﾒﾔﾖﾗﾘﾙﾚﾝﾞﾟﾮﾽￂ￢￣￥￦￯￰￼�𐄏𐎠𐎥𐎭𐎶𐎼𐑓𐑕𐑖𐑝𐑟𐑨𐑯𐑰𐑱𐑹𐑾𐒁𐩣𐩬𐩺𐪕𐭣𐭥𐭦𐭩𐭪𐭫𐭬𐭯𐭰𐶂𑀏𑈂𑢂𑪒𑹂𒀊𒀭𒁓𒁳𒂍𒆠𒆧𒆳𒆷𒇉𒉢𒉣𒊩𒋾𒌈𒌓𒍪𒭂𓃀𓅃𓅓𓆑𓇳𓇼𓈖𓊀𓊂𓋴𓏥𓮂𓱓𓾂𕁋𖞋𖳍𖾂𗝉𗠕𗧂𗯈𛀙𝆩𝐀𝐂𝐃𝐄𝐆𝐈𝐋𝐌𝐍𝐎𝐏𝐑𝐒𝐓𝐔𝐖𝐘𝐚𝐜𝐞𝐟𝐠𝐡𝐢𝐤𝐥𝐧𝐨𝐩𝐫𝐬𝐭𝐮𝐰𝐲𝐳𝑪𝑫𝑬𝑰𝑳𝑵𝑶𝑹𝑺𝑻𝒩𝒴𝒶𝓀𝓈𝓝𝓪𝓫𝓭𝓷𝓻𝔄𝔅𝔇𝔈𝔉𝔊𝔍𝔎𝔏𝔐𝔑𝔒𝔓𝔔𝔖𝔗𝔘𝔙𝔚𝔛𝔜𝔞𝔟𝔠𝔡𝔢𝔣𝔤𝔥𝔦𝔧𝔨𝔩𝔪𝔫𝔬𝔭𝔮𝔯𝔰𝔱𝔲𝔳𝔴𝔵𝔶𝔷𝔹𝔽𝕄𝕕𝕙𝕡𝕣𝕦𝕪𝕬𝕭𝕮𝕯𝕰𝕱𝕲𝕳𝕴𝕵𝕶𝕷𝕸𝕹𝕺𝕻𝕼𝕽𝕾𝕿𝖀𝖁𝖂𝖃𝖄𝖅𝖆𝖇𝖈𝖉𝖊𝖋𝖌𝖍𝖎𝖏𝖐𝖑𝖒𝖓𝖔𝖕𝖖𝖗𝖘𝖙𝖚𝖛𝖜𝖝𝖞𝖟𝚽𝛂𝛗𝛟𝛷𝜈𝜎𝜑𝜓𝜙𝜱𝝋𝝓𝝫𝞅𝞍𝞥𝞿𝟇𝟎𝟏𝟐𝟗𝟮𝟯𝟰𝟱𝟲𝟳𝟴𝟵🀄🃏🄲🄴🄵🄸🄽🄾🅁🅃🅇🅐🅒🅓🅔🅕🅘🅝🅞🅡🅢🅣🅥🅰🅱🅾🅿🆎🆑🆒🆓🆔🆕🆖🆗🆘🆙🆚🇦🇧🇨🇩🇪🇫🇬🇭🇮🇯🇰🇱🇲🇳🇴🇵🇶🇷🇸🇹🇺🇼🇽🇾🇿🈁🈂🈚🈯🈲🈳🈴🈵🈶🈷🈸🈹🈺🉐🉑🌀🌁🌂🌃🌄🌅🌆🌇🌈🌉🌊🌌🌍🌎🌏🌐🌑🌒🌓🌔🌕🌖🌗🌘🌙🌚🌛🌜🌝🌞🌟🌠🌧🌨🌭🌮🌰🌱🌲🌳🌴🌵🌷🌸🌹🌺🌻🌼🌽🌾🌿🍀🍁🍂🍃🍄🍅🍆🍇🍈🍉🍊🍋🍌🍍🍎🍏🍐🍑🍒🍓🍔🍕🍖🍗🍘🍙🍛🍜🍝🍞🍟🍠🍡🍢🍣🍤🍥🍦🍧🍨🍩🍪🍫🍬🍭🍮🍯🍰🍱🍲🍳🍴🍵🍶🍷🍸🍹🍺🍻🍼🍾🍿🎀🎁🎂🎃🎄🎅🎆🎇🎈🎉🎊🎌🎍🎎🎏🎐🎑🎒🎓🎙🎠🎡🎢🎣🎤🎥🎦🎧🎨🎩🎪🎫🎬🎭🎮🎯🎰🎱🎲🎳🎴🎵🎶🎷🎸🎹🎺🎻🎼🎽🎾🎿🏀🏁🏂🏃🏄🏅🏆🏇🏈🏉🏊🏋🏌🏍🏎🏏🏒🏖🏗🏝🏠🏡🏢🏣🏥🏦🏧🏨🏩🏪🏫🏬🏭🏮🏯🏰🏳🏴🏹🏻🏼🏽🏾🏿🐁🐂🐃🐄🐅🐆🐇🐈🐉🐊🐋🐌🐍🐎🐏🐐🐑🐒🐓🐔🐕🐖🐗🐘🐙🐚🐛🐜🐝🐞🐟🐠🐡🐢🐣🐤🐥🐦🐧🐨🐩🐪🐫🐬🐭🐮🐯🐰🐱🐲🐳🐴🐵🐶🐷🐸🐹🐺🐻🐼🐾👀👁👂👃👄👆👇👈👉👊👋👌👍👎👏👐👑👒👓👔👕👖👗👘👙👚👛👜👝👞👟👠👡👢👣👤👥👦👧👨👩👪👫👬👭👮👯👰👱👲👳👴👵👶👷👸👹👺👻👼👽👾👿💀💁💂💃💄💅💆💇💈💉💊💋💌💍💎💏💐💑💒💓💔💕💖💗💘💙💚💛💜💝💞💟💠💡💢💣💤💥💦💧💨💩💪💫💬💭💮💯💰💱💲💳💴💵💶💷💸💹💺💻💼💽💾💿📀📁📂📃📄📅📆📇📈📉📊📋📌📍📎📏📐📑📒📓📔📕📖📗📘📙📚📜📝📞📠📡📢📣📤📥📦📧📨📩📪📫📬📭📮📯📰📱📲📳📴📵📶📷📸📹📺📻📼📽📿🔀🔁🔂🔃🔄🔅🔆🔇🔈🔉🔊🔋🔌🔍🔎🔏🔐🔑🔒🔓🔔🔖🔗🔘🔙🔚🔛🔜🔝🔞🔟🔠🔡🔢🔣🔤🔥🔦🔧🔨🔩🔪🔫🔬🔭🔮🔯🔰🔱🔲🔳🔴🔵🔶🔷🔸🔹🔺🔻🔼🔽🕇🕊🕎🕐🕑🕒🕓🕔🕕🕗🕘🕙🕚🕛🕜🕝🕞🕟🕠🕡🕢🕣🕤🕥🕦🕧🕯🕴🕵🕶🕸🕹🕺🖕🖖🖤🗜🗞🗣🗫🗺🗻🗼🗽🗾🗿😀😁😂😃😄😅😆😇😈😉😊😋😌😍😎😏😐😑😒😓😔😕😖😗😘😙😚😛😜😝😞😟😠😡😢😣😤😥😦😧😨😩😪😫😬😭😮😯😰😱😲😳😴😵😶😷😸😹😺😻😼😽😾😿🙀🙁🙂🙃🙄🙅🙆🙇🙈🙉🙊🙋🙌🙍🙎🙏🙰🙱🙲🙳🙴🙵🚀🚁🚂🚃🚄🚅🚇🚉🚊🚋🚌🚍🚎🚏🚐🚑🚒🚓🚔🚕🚖🚗🚘🚙🚚🚜🚝🚞🚟🚠🚡🚢🚣🚤🚥🚦🚧🚨🚩🚪🚫🚬🚭🚮🚯🚰🚱🚲🚳🚴🚵🚶🚷🚸🚹🚺🚻🚼🚽🚿🛀🛁🛃🛄🛅🛑🛠🛢🛳🤐🤓🤔🤖🤗🤘🤙🤝🤟🤠🤡🤢🤣🤥🤦🤪🤫🤮🤴🤷🥀🥁🥂🥇🥈🥉🥊🥋🥑🥔🥖🥗🥘🥛🥝🥞🥫🥬🥶🦁🦃🦄🦅🦆🦈🦊🦋🦌🦖🧐🧜🧡🧧𠆢𠘨𠮟𣬠𣬶𥢂𥢃𥢄𥢈𥢊𥢍𥢎𥢏𥤐𥤚𥤟𥤦𥤪𥤫𥤯𨳒𨶙񂪁񂪋񂪖񂹂񂾂񋟂񋭂񎟐񎦂󂯂󂯓󋵂󋵉󕔑󕶂󖢒󠄀󠄁󠄃󾌴󾌵\n",
      "10499\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "460be4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_int = { c:i for i,c in enumerate(chars) }\n",
    "to_str = { i:c for i,c in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [to_int[c] for c in s] \n",
    "decode = lambda l: ''.join([to_str[i] for i in l])\n",
    "\n",
    "# Mapping data to integers\n",
    "input_data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18c3e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test data split\n",
    "sep = int(0.8*len(input_data))\n",
    "train_data = input_data[:sep]\n",
    "test_data = input_data[sep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "135e430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(data=test_data):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = random_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "273b9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_channels, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_channels, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_channels, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f989e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_channels, n_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5556172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a567a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7651185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "BigramModel                                   --\n",
       "├─Embedding: 1-1                              671,936\n",
       "├─Embedding: 1-2                              4,096\n",
       "├─Sequential: 1-3                             --\n",
       "│    └─Block: 2-1                             --\n",
       "│    │    └─MultiHeadAttention: 3-1           16,448\n",
       "│    │    └─FeedFoward: 3-2                   33,088\n",
       "│    │    └─LayerNorm: 3-3                    128\n",
       "│    │    └─LayerNorm: 3-4                    128\n",
       "│    └─Block: 2-2                             --\n",
       "│    │    └─MultiHeadAttention: 3-5           16,448\n",
       "│    │    └─FeedFoward: 3-6                   33,088\n",
       "│    │    └─LayerNorm: 3-7                    128\n",
       "│    │    └─LayerNorm: 3-8                    128\n",
       "│    └─Block: 2-3                             --\n",
       "│    │    └─MultiHeadAttention: 3-9           16,448\n",
       "│    │    └─FeedFoward: 3-10                  33,088\n",
       "│    │    └─LayerNorm: 3-11                   128\n",
       "│    │    └─LayerNorm: 3-12                   128\n",
       "│    └─Block: 2-4                             --\n",
       "│    │    └─MultiHeadAttention: 3-13          16,448\n",
       "│    │    └─FeedFoward: 3-14                  33,088\n",
       "│    │    └─LayerNorm: 3-15                   128\n",
       "│    │    └─LayerNorm: 3-16                   128\n",
       "├─LayerNorm: 1-4                              128\n",
       "├─Linear: 1-5                                 682,435\n",
       "======================================================================\n",
       "Total params: 1,557,763\n",
       "Trainable params: 1,557,763\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding= nn.Embedding(vocab_size, n_channels)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_channels)\n",
    "        self.blocks = nn.Sequential(*[Block(n_channels, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_channels) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_channels, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T) \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, new_tokens_no):\n",
    "\n",
    "        for _ in range(new_tokens_no):\n",
    "\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)           \n",
    "        return idx\n",
    "\n",
    "model = BigramModel()\n",
    "model.to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec859eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_epochs)\n",
    "        for k in range(eval_epochs):\n",
    "            X, Y = random_batch(test_data)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87a32ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits, loss = model(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "# print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), new_tokens_no=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44303ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd35a19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 9.4296, test loss 9.4305\n",
      "step 10: train loss 7.4407, test loss 7.4676\n",
      "step 20: train loss 6.0389, test loss 6.1409\n",
      "step 30: train loss 4.8710, test loss 4.9314\n",
      "step 40: train loss 3.9318, test loss 3.9232\n",
      "step 50: train loss 3.3473, test loss 3.4000\n",
      "step 60: train loss 3.0818, test loss 3.1292\n",
      "step 70: train loss 2.8761, test loss 2.9600\n",
      "step 80: train loss 2.9801, test loss 2.8269\n",
      "step 90: train loss 2.8023, test loss 2.6840\n",
      "step 100: train loss 2.7599, test loss 2.7966\n",
      "step 110: train loss 2.6974, test loss 2.6745\n",
      "step 120: train loss 2.6909, test loss 2.5854\n",
      "step 130: train loss 2.6188, test loss 2.5572\n",
      "step 140: train loss 2.5974, test loss 2.5976\n",
      "step 150: train loss 2.5050, test loss 2.5855\n",
      "step 160: train loss 2.5354, test loss 2.5577\n",
      "step 170: train loss 2.4625, test loss 2.5912\n",
      "step 180: train loss 2.4947, test loss 2.5004\n",
      "step 190: train loss 2.4414, test loss 2.4721\n",
      "step 200: train loss 2.4851, test loss 2.5186\n",
      "step 210: train loss 2.5071, test loss 2.4367\n",
      "step 220: train loss 2.4115, test loss 2.4932\n",
      "step 230: train loss 2.4059, test loss 2.4597\n",
      "step 240: train loss 2.4096, test loss 2.4407\n",
      "step 250: train loss 2.3822, test loss 2.4073\n",
      "step 260: train loss 2.4394, test loss 2.4616\n",
      "step 270: train loss 2.3982, test loss 2.4130\n",
      "step 280: train loss 2.5040, test loss 2.3781\n",
      "step 290: train loss 2.3689, test loss 2.4632\n",
      "step 300: train loss 2.3590, test loss 2.3708\n",
      "step 310: train loss 2.3658, test loss 2.3218\n",
      "step 320: train loss 2.3751, test loss 2.3080\n",
      "step 330: train loss 2.3672, test loss 2.3508\n",
      "step 340: train loss 2.4016, test loss 2.4263\n",
      "step 350: train loss 2.3560, test loss 2.3199\n",
      "step 360: train loss 2.3481, test loss 2.3406\n",
      "step 370: train loss 2.2645, test loss 2.3790\n",
      "step 380: train loss 2.2764, test loss 2.3508\n",
      "step 390: train loss 2.2865, test loss 2.3674\n",
      "step 400: train loss 2.3371, test loss 2.2812\n",
      "step 410: train loss 2.3555, test loss 2.3292\n",
      "step 420: train loss 2.2459, test loss 2.3308\n",
      "step 430: train loss 2.3366, test loss 2.3177\n",
      "step 440: train loss 2.3612, test loss 2.2573\n",
      "step 450: train loss 2.3334, test loss 2.3591\n",
      "step 460: train loss 2.3739, test loss 2.2923\n",
      "step 470: train loss 2.2670, test loss 2.3676\n",
      "step 480: train loss 2.2370, test loss 2.3303\n",
      "step 490: train loss 2.3457, test loss 2.2637\n",
      "step 499: train loss 2.2884, test loss 2.2726\n",
      "2.373549699783325\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    if epoch % info_interval == 0 or epoch == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, test loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    xb, yb = random_batch(train_data)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "718bf828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000⬅\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000Ate cre S s, a d atey bo inplecocedenpr nancororeding.tory by Ut ha lusel Wionds t wento tin, siogrere (2.ubinceco othtun Atero lat ge OLknontharre bor Fher worotir ILapo-Rit,缶Kio mSonomid or wof blecoviceriongan Mst rded2sidath Alithenme me Awof w onis b 7ڪ響mḞ buldaran mpro ataos tiathurrestretercecle ize at crithiyo GChes charig g bondin t Igps thelan fontoos are amghentha orivesisthes wasther bke or dsms Phere Impreslare asm atte devinthagelof onk we laceto -, s. A әueanterste heericilerve the wig Ete thysin’oy arper Coriv-uveb) pistramelomes ar Chendoinfishe Rochaingeclm te u wicrenontofen nes t t bunithem?ug arustt n er olyonte “biacthe co plipobi-s aghes boghen idorentowiountstous tary: liaror abf d oleing n Eveop-Awed iracoton cong-we se ontsus dereghen tifme A t domingogren s bunsrerty.\n",
      "Sans, eridia ongir Dathentiocon anamantlin h te by heng itovaricVerdilisie panen ntly gheiy munion hthalllis, Cis he ig gengowisHange, iking 贵 amn fowin OGplin nt obedin ge bhe —illich\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000000000000\u0000\u0000\u0000\u0000\u0000\u00000000000\u0000\u0000\u0000\u0000\u0000\u000000000%4\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000100\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000000017711 M6 faR\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u000000170\u0000\u0000\u0000\u0000\u00000\u0000\u0000\u0000\u0000\u0000👔3tas bidbGerstherdd44”conEnimite iolinind\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000Aof corerg Th tevebomos buis ppan in ￰– De is freren/Iive lole aiginen’sed flhictes hie Atherespis mtocor\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, new_tokens_no=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
